# -*- coding: utf-8 -*-
"""DeepLearningAssignment3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11idEWBOfGsEAClCBrfjEZywyOO7_93N9
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

class StockDataset:
    def __init__(self, filepath, time_steps=30, val_ratio=0.2, test_ratio=0.2):
        self.filepath = filepath
        self.time_steps = time_steps
        self.val_ratio = val_ratio
        self.test_ratio = test_ratio


        self.data = pd.read_csv(filepath)
        self.raw_seq = self.data['ClosePrice'].values.reshape(-1, 1)

        self.scaler = MinMaxScaler()
        self.normalized_seq = self.scaler.fit_transform(self.raw_seq)

        # Split dataset
        self.X_train, self.y_train, self.X_val, self.y_val, self.X_test, self.y_test = self._prepare_data()
    def _create_sequences(self, data):
        X, y = [], []
        for i in range(len(data) - self.time_steps):
            X.append(data[i:i + self.time_steps])
            y.append(data[i + self.time_steps])
        return np.array(X), np.array(y)

    def _prepare_data(self):
        total_len = len(self.normalized_seq)
        test_size = int(total_len * self.test_ratio)
        val_size = int(total_len * self.val_ratio)

        train_data = self.normalized_seq[:-(test_size + val_size)]
        val_data = self.normalized_seq[-(test_size + val_size):-test_size]
        test_data = self.normalized_seq[-test_size:]

        X_train, y_train = self._create_sequences(train_data)
        X_val, y_val = self._create_sequences(val_data)
        X_test, y_test = self._create_sequences(test_data)

        return X_train, y_train, X_val, y_val, X_test, y_test

from tensorflow.keras.layers import Input
#input layer
def build_model(model_type, input_shape, rnn_size=128, dropout=0.2):
    model = Sequential()
    model.add(Input(shape=input_shape))
    if model_type == 'VanillaRNN':
        model.add(SimpleRNN(rnn_size, activation='tanh'))
    elif model_type == 'LSTM':
        model.add(LSTM(rnn_size, activation='tanh'))
    elif model_type == 'GRU':
        model.add(GRU(rnn_size, activation='tanh'))
    else:
        raise ValueError("Invalid model type. Choose from 'VanillaRNN', 'LSTM', 'GRU'")
    model.add(Dropout(dropout))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

def train_model(model, X_train, y_train, X_val, y_val, batch_size=64, epochs=50):

    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)
    model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, save_weights_only=False)

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stopping, reduce_lr, model_checkpoint]
    )
    return history

def evaluate_model(model, X_test, y_test, scaler):
    # predict
    y_pred = model.predict(X_test)
    y_pred_rescaled = scaler.inverse_transform(y_pred)
    y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))


    mse = mean_squared_error(y_test_rescaled, y_pred_rescaled)
    rmse = np.sqrt(mse)


    print(f"Test MSE: {mse:.4f}")
    print(f"Test RMSE: {rmse:.4f}")


    #  MDA
    y_test_diff = np.sign(np.diff(y_test_rescaled.flatten()))
    y_pred_diff = np.sign(np.diff(y_pred_rescaled.flatten()))
    mda = np.mean(y_test_diff == y_pred_diff)
    print(f"Test MDA: {mda:.4f}")

def plot_results(history, y_test, y_pred, scaler):

    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    y_pred_rescaled = scaler.inverse_transform(y_pred)
    y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))
    plt.figure(figsize=(12, 6))
    plt.plot(y_test_rescaled, label='True Values')
    plt.plot(y_pred_rescaled, label='Predictions')
    plt.title('True vs Predicted Values')
    plt.legend()
    plt.show()

from google.colab import files
uploaded = files.upload()

if __name__ == "__main__":

    filepath = "SH300IFcombined.csv"
    model_type = "VanillaRNN"


    dataset = StockDataset(filepath, time_steps=30)


    input_shape = (dataset.X_train.shape[1], dataset.X_train.shape[2])
    model = build_model(model_type, input_shape)


    history = train_model(model, dataset.X_train, dataset.y_train, dataset.X_val, dataset.y_val)


    evaluate_model(model, dataset.X_test, dataset.y_test, dataset.scaler)


    plot_results(history, dataset.y_test, model.predict(dataset.X_test), dataset.scaler)

if __name__ == "__main__":

    filepath = "SH300IFcombined.csv"
    model_type = "LSTM"


    dataset = StockDataset(filepath, time_steps=30)

    input_shape = (dataset.X_train.shape[1], dataset.X_train.shape[2])
    model = build_model(model_type, input_shape)


    history = train_model(model, dataset.X_train, dataset.y_train, dataset.X_val, dataset.y_val)


    evaluate_model(model, dataset.X_test, dataset.y_test, dataset.scaler)

    plot_results(history, dataset.y_test, model.predict(dataset.X_test), dataset.scaler)

if __name__ == "__main__":

    filepath = "SH300IFcombined.csv"
    model_type = "GRU"


    dataset = StockDataset(filepath, time_steps=30)


    input_shape = (dataset.X_train.shape[1], dataset.X_train.shape[2])
    model = build_model(model_type, input_shape)


    history = train_model(model, dataset.X_train, dataset.y_train, dataset.X_val, dataset.y_val)


    evaluate_model(model, dataset.X_test, dataset.y_test, dataset.scaler)


    plot_results(history, dataset.y_test, model.predict(dataset.X_test), dataset.scaler)



from itertools import product


param_grid = {
    "rnn_size": [64, 128],
    "time_steps": [30, 50],
    "dropout": [0.2, 0.3],
    "learning_rate": [0.001, 0.0005],
    "batch_size": [64, 128]
}


param_combinations = list(product(
    param_grid["rnn_size"],
    param_grid["time_steps"],
    param_grid["dropout"],
    param_grid["learning_rate"],
    param_grid["batch_size"]
))

from tensorflow.keras.layers import GRU, Dropout, Dense, Input
from tensorflow.keras.models import Sequential
from tensorflow.keras import optimizers

def train_and_evaluate(params, dataset):

    rnn_size, time_steps, dropout, learning_rate, batch_size = params

    dataset.time_steps = time_steps
    dataset.X_train, dataset.y_train, dataset.X_val, dataset.y_val, dataset.X_test, dataset.y_test = dataset._prepare_data()

    model = Sequential()
    model.add(Input(shape=(dataset.X_train.shape[1], dataset.X_train.shape[2])))
    model.add(GRU(rnn_size, activation='tanh'))
    model.add(Dropout(dropout))
    model.add(Dense(1))


    optimizer = optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse')


    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(
        dataset.X_train, dataset.y_train,
        validation_data=(dataset.X_val, dataset.y_val),
        epochs=50,
        batch_size=batch_size,
        callbacks=[early_stopping],
        verbose=0
    )


    val_loss = min(history.history['val_loss'])
    return val_loss, model

best_params = None
best_val_loss = float('inf')
best_model = None

for params in param_combinations:
    print(f"Training with params: {params}")
    val_loss, model = train_and_evaluate(params, dataset)
    print(f"Validation Loss: {val_loss}")

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_params = params
        best_model = model

print(f"Best Params: {best_params}")
print(f"Best Validation Loss: {best_val_loss}")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


results = []


for params in param_combinations:
    print(f"Training with params: {params}")
    val_loss, _ = train_and_evaluate(params, dataset)
    print(f"Validation Loss: {val_loss}")

    results.append({"rnn_size": params[0],
                    "time_steps": params[1],
                    "dropout": params[2],
                    "learning_rate": params[3],
                    "batch_size": params[4],
                    "val_loss": val_loss})


results_df = pd.DataFrame(results)
print(results_df)

sorted_results = results_df.sort_values(by="val_loss", ascending=True)


plt.figure(figsize=(12, 6))
plt.barh(
    [f"RNN: {row.rnn_size}, TS: {row.time_steps}, DO: {row.dropout}, LR: {row.learning_rate}, BS: {row.batch_size}"
     for _, row in sorted_results.iterrows()],
    sorted_results["val_loss"],
    color='skyblue'
)
plt.xlabel("Validation Loss", fontsize=12)
plt.ylabel("Parameter Combination", fontsize=12)
plt.title("Validation Loss for Different Hyperparameter Combinations", fontsize=14)
plt.gca().invert_yaxis()
plt.show()

def train_and_evaluate(params, dataset):

    rnn_size, time_steps, dropout, learning_rate, batch_size = params


    dataset.time_steps = time_steps
    dataset.X_train, dataset.y_train, dataset.X_val, dataset.y_val, dataset.X_test, dataset.y_test = dataset._prepare_data()


    model = Sequential()
    model.add(Input(shape=(dataset.X_train.shape[1], dataset.X_train.shape[2])))  # 输入层
    model.add(GRU(rnn_size, activation='tanh'))
    model.add(Dropout(dropout))
    model.add(Dense(1))


    optimizer = optimizers.SGD(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse')


    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


    history = model.fit(
        dataset.X_train, dataset.y_train,
        validation_data=(dataset.X_val, dataset.y_val),
        epochs=50,
        batch_size=batch_size,
        callbacks=[early_stopping],
        verbose=0
    )


    val_loss = min(history.history['val_loss'])
    return val_loss, model

results = []


for params in param_combinations:
    print(f"Training with params: {params}")
    val_loss, _ = train_and_evaluate(params, dataset)
    print(f"Validation Loss: {val_loss}")
    results.append({"rnn_size": params[0],
                    "time_steps": params[1],
                    "dropout": params[2],
                    "learning_rate": params[3],
                    "batch_size": params[4],
                    "val_loss": val_loss})


results_df = pd.DataFrame(results)
print(results_df)

sorted_results = results_df.sort_values(by="val_loss", ascending=True)


plt.figure(figsize=(12, 6))
plt.barh(
    [f"RNN: {row.rnn_size}, TS: {row.time_steps}, DO: {row.dropout}, LR: {row.learning_rate}, BS: {row.batch_size}"
     for _, row in sorted_results.iterrows()],
    sorted_results["val_loss"],
    color='skyblue'
)
plt.xlabel("Validation Loss", fontsize=12)
plt.ylabel("Parameter Combination", fontsize=12)
plt.title("Validation Loss for Different Hyperparameter Combinations", fontsize=14)
plt.gca().invert_yaxis()
plt.show()



from tensorflow.keras.layers import GRU, Dropout, Dense, Input
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

def build_gru_model(input_shape, rnn_size=128, dropout=0.2, learning_rate=0.001):

    model = Sequential()
    model.add(Input(shape=input_shape))
    model.add(GRU(rnn_size, activation='relu'))
    model.add(Dropout(dropout))
    model.add(Dense(1))


    optimizer = tanh(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse')
    return model

input_shape = (dataset.X_train.shape[1], dataset.X_train.shape[2])  # 输入形状
model = build_gru_model(input_shape, rnn_size=128, dropout=0.2, learning_rate=0.001)


early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


history = model.fit(
    dataset.X_train, dataset.y_train,
    validation_data=(dataset.X_val, dataset.y_val),
    epochs=50,
    batch_size=64,
    callbacks=[early_stopping]
)


model.summary()

import matplotlib.pyplot as plt


plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs', fontsize=12)
plt.ylabel('Loss (MSE)', fontsize=12)
plt.title('Training and Validation Loss (ReLU Activation)', fontsize=14)
plt.legend()
plt.show()